{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analyze-word-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncp-Bd1tUtPK",
        "colab_type": "text"
      },
      "source": [
        "# Analysing performance of word2vec, tf-idf and GloVe on text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSSG18vru1xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get data from US CFB website for consumer complaints\n",
        "!wget https://data.consumerfinance.gov/api/views/s6ew-h6mp/rows.csv?accessType=DOWNLOAD\n",
        "# rename file\n",
        "!mv /content/rows.csv?accessType=DOWNLOAD /content/rows.csv\n",
        "\n",
        "#downlowad word2vec pretrained word embeddings from Spacy\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.0/en_core_web_md-2.2.0.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSotQBaexE_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import spacy\n",
        "import nltk\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "pd.set_option('max_colwidth',1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUseW0awxVsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read from file and remove NA/Null values from dataframe\n",
        "dir_path = '/content/'\n",
        "\n",
        "df = pd.read_csv(os.path.join(dir_path, 'rows.csv'))\n",
        "df = df[pd.notnull(df['Consumer complaint narrative'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QywZ6fS4xnJG",
        "colab_type": "code",
        "outputId": "47e53e58-74f9-4c01-95a8-59dd4447aa5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 481948 entries, 5 to 1474449\n",
            "Data columns (total 18 columns):\n",
            "Date received                   481948 non-null object\n",
            "Product                         481948 non-null object\n",
            "Sub-product                     429776 non-null object\n",
            "Issue                           481948 non-null object\n",
            "Sub-issue                       355148 non-null object\n",
            "Consumer complaint narrative    481948 non-null object\n",
            "Company public response         234297 non-null object\n",
            "Company                         481948 non-null object\n",
            "State                           480095 non-null object\n",
            "ZIP code                        373611 non-null object\n",
            "Tags                            82657 non-null object\n",
            "Consumer consent provided?      481948 non-null object\n",
            "Submitted via                   481948 non-null object\n",
            "Date sent to company            481948 non-null object\n",
            "Company response to consumer    481947 non-null object\n",
            "Timely response?                481948 non-null object\n",
            "Consumer disputed?              164067 non-null object\n",
            "Complaint ID                    481948 non-null int64\n",
            "dtypes: int64(1), object(17)\n",
            "memory usage: 69.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71-dcWn1xqYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The analysis is on text classification, so the columns that are important are \n",
        "#Product(labels) and Consumer Complaint Narrative (complaints from consumer)\n",
        "df = df[['Product','Consumer complaint narrative']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMGHyGSvOund",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Working on a subset of the problem for faster processing\n",
        "df = df[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9oEht-cxy76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the pretrained model for English langauge from Spacy and stopwords\n",
        "import en_core_web_md\n",
        "\n",
        "nlp = en_core_web_md.load()\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdywhBbJy9al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compaints are docs/text and categories are tags\n",
        "complaints = df['Consumer complaint narrative']\n",
        "categories = df['Product']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYi8qTuRzCsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing for lowercase, tokenization and removing accents from characters\n",
        "#complaint_ids = np.arange(len(complaints))\n",
        "complaints_tokens = [gensim.utils.simple_preprocess(comp, deacc=True) for comp in complaints]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD2ZBAUEzTMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting back the tokenized complaints into sentence because the pretrained English word model \n",
        "#takes input as string (sentences) and not in tokenzied form\n",
        "simple_complaints = []\n",
        "for tokens in complaints_tokens:\n",
        "\t\t\tsimple = \" \".join(tokens)  # concatenate back to a sentence\n",
        "\t\t\tsimple_complaints.append(simple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCHEk5Ba0Qe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allowed_postags=['ADV', 'VERB', 'ADJ', 'NOUN', 'PROPN', 'NUM']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YquDZoZV3zB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmataizing text using English word model and removing POS not allowed and stopwords\n",
        "complaints_words = [] #lemmatized\n",
        "count=0\n",
        "for each_complaint in simple_complaints:\n",
        "  each_complaint_nlp = nlp(each_complaint)\n",
        "  tokens = [token.lemma_ for token in each_complaint_nlp if (token.pos_ in allowed_postags) and (token.text not in stop_words)]\n",
        "  complaints_words.append(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgRK6ghXPCW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the lemmatized form as pickle because it takes a lot of time for processing\n",
        "import pickle\n",
        "\n",
        "# Save all_docs as pickle.\n",
        "with open(os.path.join(dir_path, 'complaints_words_2.pickle'), 'wb') as f:\n",
        "    pickle.dump(complaints_words, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILFHe83RPG1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read pickle.\n",
        "with open(os.path.join(dir_path, 'complaints_words_2.pickle'), 'rb') as f:\n",
        "    complaints_words = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGAGCkWM44h-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#used for doc2word so not important right now\n",
        "#complaint_ids = np.arange(len(complaints))\n",
        "#all_complaints = [TaggedDocument(words=words, tags=[tag]) for words, tag in zip(complaints_words, complaint_ids)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Nro5Lh2Ms0",
        "colab_type": "code",
        "outputId": "c8533a7f-05b8-44c7-e39a-a66cd3b9c52f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#word2vec\n",
        "import multiprocessing\n",
        "import sys\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "workers = multiprocessing.cpu_count()\n",
        "print('number of cpu: {}'.format(workers))\n",
        "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise.\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of cpu: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V63HjBYyoiLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating word2vec model with 100 dimensions\n",
        "word_model = Word2Vec(complaints_words,\n",
        "                      min_count=2,\n",
        "                      size=100,\n",
        "                      window=5,\n",
        "                      workers=workers,\n",
        "                      iter=100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGXvJWfrrLYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word averaging for each sentence for weighted word2vec\n",
        "# Returns the weighted mean of the sentence from word vectors in the sentence, else zero\n",
        "def word_average(sentence):\n",
        "  mean_doc = []\n",
        "\n",
        "  for word in sentence:\n",
        "    if word in word_model.wv.vocab:\n",
        "      mean_doc.append(word_model.wv.get_vector(word))\n",
        "      pass\n",
        "    pass\n",
        "    \n",
        "  if not mean_doc:\n",
        "    return np.zeros(word_model.wv.vector_size)\n",
        "  else:\n",
        "    mean_doc = np.array(mean_doc).mean(axis=0)\n",
        "    pass\n",
        "\n",
        "  return mean_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQiglPNKU7Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Averaging word2vec embedding for each document/complaint\n",
        "weighted_word2vec_complaint_vector = np.vstack([word_average(sent) for sent in complaints_words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqEW3XwIzIPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TF_IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import namedtuple, defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7nRF0RjqnK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate idf weights for each word\n",
        "word_idf_weight = []\n",
        "#Converting the lemmatized words into sentences for TFidf vectorizer input\n",
        "complaints_setences = [\" \".join(doc) for doc in complaints_words]\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit(complaints_setences) #accepts sentence strings as an array\n",
        "\n",
        "#For words with idf as zero, maximum IDF value is assigned for equal consideration (no bias) as most known word\n",
        "max_idf = max(tfidf.idf_)\n",
        "word_idf_weight = defaultdict(lambda: max_idf,\n",
        "\t\t\t\t\t\t\t\t\t\t   [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DmcFIxozHc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#utility function to combine the weighted word2vec (term frequency) and idf weight for each sentence\n",
        "def word_average_with_idf(sentence):\n",
        "  mean_doc = []\n",
        "\n",
        "  for word in sentence:\n",
        "    if word in word_model.wv.vocab:\n",
        "      mean_doc.append(word_model.wv.get_vector(word) * word_idf_weight[word])\n",
        "      pass\n",
        "    pass\n",
        "    \n",
        "  if not mean_doc:\n",
        "    return np.zeros(word_model.wv.vector_size)\n",
        "  else:\n",
        "    mean_doc = np.array(mean_doc).mean(axis=0)\n",
        "    pass\n",
        "\n",
        "  return mean_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upn3WwUCy44p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Averaging tfidf weighted embedding for each document/complaint\n",
        "weighted_tfidf_complaint_vector = np.vstack([word_average_with_idf(sent) for sent in complaints_words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH9LzHoT5vC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GloVe\n",
        "\n",
        "from gensim.test.utils import get_tmpfile, datapath\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import gensim.downloader as api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv4zRON0JHLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3173fc92-66b9-4d04-d82f-e14d01ae90aa"
      },
      "source": [
        "#Download glove pretrained embedding from the GloVe website\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip '/content/glove.twitter.27B.zip'\n",
        "!mv /content/glove.twitter.27B/glove.6B.100d.txt ../glove.6B.100d.txt"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/glove.twitter.27B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/glove.twitter.27B.zip or\n",
            "        /content/glove.twitter.27B.zip.zip, and cannot find /content/glove.twitter.27B.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g62dtXEkHHqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load in GloVe vector\n",
        "glove_vec_fi = datapath('/content/glove.6B.100d.txt')\n",
        "tmp_word2vec_fi = get_tmpfile('tmp_glove2word2vec.txt')\n",
        "\n",
        "#Convert glove representation into word2vec format and store it in tmp_word2vec_fi\n",
        "glove2word2vec(glove_vec_fi, tmp_word2vec_fi)\n",
        "\n",
        "#Read the stored tmp_word2vec_fi file to load glove in word2vec format\n",
        "glove_word_model = KeyedVectors.load_word2vec_format(tmp_word2vec_fi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf7tEXyPKwyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word averaging for each sentence based upon GloVe embeddings:\n",
        "\n",
        "def word_average_glove(sentence):\n",
        "  mean_doc = []\n",
        "\n",
        "  for word in sentence:\n",
        "    if word in glove_word_model.wv.vocab:\n",
        "      mean_doc.append(glove_word_model.wv.get_vector(word))\n",
        "      pass\n",
        "    pass\n",
        "    \n",
        "  if not mean_doc:\n",
        "    return np.zeros(glove_word_model.wv.vector_size)\n",
        "  else:\n",
        "    mean_doc = np.array(mean_doc).mean(axis=0)\n",
        "    pass\n",
        "\n",
        "  return mean_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA69ObHRLhKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Averaging word2vec embeddings for each document/complaint\n",
        "weighted_glove_complaint_vector = np.vstack([word_average_glove(sent) for sent in complaints_words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDh-OM9OLpmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7172b32-fec7-4e79-9a98-eb33fe8a3fc2"
      },
      "source": [
        "#weighted_idf_word2vec_complaint_vector.shape\n",
        "#weighted_word2vec_complaint_vector.shape\n",
        "#weighted_glove_complaint_vector.shape"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1pMY13nMQ6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Classification via Logistic Model\n",
        "logistic = LogisticRegression(random_state=1, multi_class='multinomial', solver='saga')\n",
        "\n",
        "# Stochastic Gradient Descent classifier\n",
        "sgd = SGDClassifier(loss='hinge',\n",
        "                    verbose=1,\n",
        "                    random_state=1,\n",
        "                    learning_rate='invscaling',\n",
        "                    eta0=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtnqLP7aM19G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = weighted_word2vec_complaint_vector\n",
        "#df = weighted_idf_word2vec_complaint_vector\n",
        "#df = weighted_glove_complaint_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ntjxqq3OCgw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a437ef01-9226-49a7-95be-9b87a1bea8c2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "model = logisitic\n",
        "\n",
        "train_size = math.floor(len(df) * 0.8)\n",
        "test_size = len(df) - train_size\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(df,\n",
        "                                                    categories,\n",
        "                                                     test_size=test_size,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=categories)\n",
        "\n",
        "print('Shape of train_X: {}'.format(train_X.shape))\n",
        "print('Shape of text_X: {}'.format(test_X.shape))\n",
        "\n",
        "model.fit(train_X, train_y)\n",
        "\n",
        "pred = model.predict(train_X)\n",
        "true = np.array(train_y)\n",
        "\n",
        "print('word2vec Score on Training dataset...\\n')\n",
        "print('Confusion Matrix:\\n', confusion_matrix(true, pred))\n",
        "print('\\nClassification Report:\\n', classification_report(true, pred, target_names=None))\n",
        "print('\\naccuracy: {:.3f}'.format(accuracy_score(true, pred)))\n",
        "print('f1 score: {:.3f}'.format(f1_score(true, pred, average='weighted')))\n",
        "\n",
        "pred_test = model.predict(test_X)\n",
        "true_test = np.array(test_y)\n",
        "\n",
        "print('word2vec Score on testing dataset...\\n')\n",
        "print('\\naccuracy: {:.3f}'.format(accuracy_score(true_test, pred_test)))\n",
        "print('f1 score: {:.3f}'.format(f1_score(true_test, pred_test, average='weighted')))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_X: (8000, 100)\n",
            "Shape of text_X: (2000, 100)\n",
            "-- Epoch 1\n",
            "Norm: 4.62, NNZs: 100, Bias: -2.972238, T: 8000, Avg. loss: 0.204910\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3.89, NNZs: 100, Bias: -2.795986, T: 16000, Avg. loss: 0.119723\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 3.53, NNZs: 100, Bias: -2.676746, T: 24000, Avg. loss: 0.102552\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 3.28, NNZs: 100, Bias: -2.560855, T: 32000, Avg. loss: 0.097128\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 3.12, NNZs: 100, Bias: -2.400735, T: 40000, Avg. loss: 0.093413\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2.98, NNZs: 100, Bias: -2.303200, T: 48000, Avg. loss: 0.090969\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2.84, NNZs: 100, Bias: -2.246695, T: 56000, Avg. loss: 0.088850\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2.76, NNZs: 100, Bias: -2.136931, T: 64000, Avg. loss: 0.087167\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 2.66, NNZs: 100, Bias: -2.103351, T: 72000, Avg. loss: 0.086438\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 2.58, NNZs: 100, Bias: -2.062931, T: 80000, Avg. loss: 0.085398\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 2.52, NNZs: 100, Bias: -1.997138, T: 88000, Avg. loss: 0.083984\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 2.46, NNZs: 100, Bias: -1.941014, T: 96000, Avg. loss: 0.083659\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2.40, NNZs: 100, Bias: -1.903140, T: 104000, Avg. loss: 0.083080\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2.34, NNZs: 100, Bias: -1.866555, T: 112000, Avg. loss: 0.082400\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 2.29, NNZs: 100, Bias: -1.837425, T: 120000, Avg. loss: 0.082626\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 2.26, NNZs: 100, Bias: -1.772449, T: 128000, Avg. loss: 0.081210\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 2.21, NNZs: 100, Bias: -1.770142, T: 136000, Avg. loss: 0.081837\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 2.16, NNZs: 100, Bias: -1.767323, T: 144000, Avg. loss: 0.080987\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 2.13, NNZs: 100, Bias: -1.715053, T: 152000, Avg. loss: 0.080901\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 2.09, NNZs: 100, Bias: -1.702584, T: 160000, Avg. loss: 0.080486\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 2.05, NNZs: 100, Bias: -1.675413, T: 168000, Avg. loss: 0.080462\n",
            "Total training time: 0.07 seconds.\n",
            "Convergence after 21 epochs took 0.07 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2.87, NNZs: 100, Bias: -2.022409, T: 8000, Avg. loss: 0.126310\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.68, NNZs: 100, Bias: -1.935968, T: 16000, Avg. loss: 0.093521\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.56, NNZs: 100, Bias: -1.850821, T: 24000, Avg. loss: 0.089531\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2.49, NNZs: 100, Bias: -1.809513, T: 32000, Avg. loss: 0.086458\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2.42, NNZs: 100, Bias: -1.771454, T: 40000, Avg. loss: 0.085914\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2.38, NNZs: 100, Bias: -1.733575, T: 48000, Avg. loss: 0.083801\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2.33, NNZs: 100, Bias: -1.699360, T: 56000, Avg. loss: 0.083910\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2.31, NNZs: 100, Bias: -1.671331, T: 64000, Avg. loss: 0.082532\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 2.29, NNZs: 100, Bias: -1.653132, T: 72000, Avg. loss: 0.082330\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 2.26, NNZs: 100, Bias: -1.653282, T: 80000, Avg. loss: 0.081861\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 2.23, NNZs: 100, Bias: -1.656581, T: 88000, Avg. loss: 0.081753\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 2.21, NNZs: 100, Bias: -1.653007, T: 96000, Avg. loss: 0.081930\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2.19, NNZs: 100, Bias: -1.630374, T: 104000, Avg. loss: 0.081353\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 13 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 3.36, NNZs: 100, Bias: -1.773333, T: 8000, Avg. loss: 0.106499\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.94, NNZs: 100, Bias: -1.729351, T: 16000, Avg. loss: 0.058442\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.68, NNZs: 100, Bias: -1.676555, T: 24000, Avg. loss: 0.052186\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2.49, NNZs: 100, Bias: -1.632840, T: 32000, Avg. loss: 0.048813\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2.34, NNZs: 100, Bias: -1.574940, T: 40000, Avg. loss: 0.047136\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2.21, NNZs: 100, Bias: -1.527332, T: 48000, Avg. loss: 0.046022\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2.10, NNZs: 100, Bias: -1.510064, T: 56000, Avg. loss: 0.044796\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2.01, NNZs: 100, Bias: -1.484066, T: 64000, Avg. loss: 0.043683\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.92, NNZs: 100, Bias: -1.441198, T: 72000, Avg. loss: 0.043256\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.84, NNZs: 100, Bias: -1.423670, T: 80000, Avg. loss: 0.042697\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1.77, NNZs: 100, Bias: -1.399238, T: 88000, Avg. loss: 0.042377\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1.70, NNZs: 100, Bias: -1.372470, T: 96000, Avg. loss: 0.041669\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1.64, NNZs: 100, Bias: -1.362916, T: 104000, Avg. loss: 0.041463\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 13 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2.84, NNZs: 100, Bias: -1.372088, T: 8000, Avg. loss: 0.190087\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.23, NNZs: 100, Bias: -1.328913, T: 16000, Avg. loss: 0.128601\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1.90, NNZs: 100, Bias: -1.225086, T: 24000, Avg. loss: 0.118178\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.65, NNZs: 100, Bias: -1.171325, T: 32000, Avg. loss: 0.114039\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.48, NNZs: 100, Bias: -1.145545, T: 40000, Avg. loss: 0.110269\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.32, NNZs: 100, Bias: -1.140482, T: 48000, Avg. loss: 0.108908\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.21, NNZs: 100, Bias: -1.108347, T: 56000, Avg. loss: 0.106780\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.12, NNZs: 100, Bias: -1.087003, T: 64000, Avg. loss: 0.105686\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.03, NNZs: 100, Bias: -1.028798, T: 72000, Avg. loss: 0.105109\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.96, NNZs: 100, Bias: -0.999675, T: 80000, Avg. loss: 0.104022\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.90, NNZs: 100, Bias: -0.992779, T: 88000, Avg. loss: 0.103369\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.85, NNZs: 100, Bias: -0.979651, T: 96000, Avg. loss: 0.102780\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Norm: 0.80, NNZs: 100, Bias: -0.970464, T: 104000, Avg. loss: 0.102639\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.75, NNZs: 100, Bias: -0.951973, T: 112000, Avg. loss: 0.101967\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 0.72, NNZs: 100, Bias: -0.928730, T: 120000, Avg. loss: 0.101540\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 15 epochs took 0.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2.53, NNZs: 100, Bias: -1.550156, T: 8000, Avg. loss: 0.185460\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.27, NNZs: 100, Bias: -1.530823, T: 16000, Avg. loss: 0.146517\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.13, NNZs: 100, Bias: -1.479567, T: 24000, Avg. loss: 0.139781\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2.03, NNZs: 100, Bias: -1.467901, T: 32000, Avg. loss: 0.137907\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.97, NNZs: 100, Bias: -1.438451, T: 40000, Avg. loss: 0.134821\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.92, NNZs: 100, Bias: -1.394469, T: 48000, Avg. loss: 0.133828\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.86, NNZs: 100, Bias: -1.386041, T: 56000, Avg. loss: 0.133663\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.81, NNZs: 100, Bias: -1.378845, T: 64000, Avg. loss: 0.132589\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.78, NNZs: 100, Bias: -1.356593, T: 72000, Avg. loss: 0.131582\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.74, NNZs: 100, Bias: -1.367653, T: 80000, Avg. loss: 0.131299\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1.72, NNZs: 100, Bias: -1.339955, T: 88000, Avg. loss: 0.130548\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1.68, NNZs: 100, Bias: -1.310408, T: 96000, Avg. loss: 0.131048\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1.64, NNZs: 100, Bias: -1.310975, T: 104000, Avg. loss: 0.130796\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1.62, NNZs: 100, Bias: -1.262785, T: 112000, Avg. loss: 0.129999\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 14 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 4.26, NNZs: 100, Bias: -2.698736, T: 8000, Avg. loss: 0.337439\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3.34, NNZs: 100, Bias: -2.365018, T: 16000, Avg. loss: 0.222028\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.83, NNZs: 100, Bias: -2.153553, T: 24000, Avg. loss: 0.198390\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2.48, NNZs: 100, Bias: -1.955942, T: 32000, Avg. loss: 0.188151\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2.20, NNZs: 100, Bias: -1.853931, T: 40000, Avg. loss: 0.180595\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2.02, NNZs: 100, Bias: -1.714081, T: 48000, Avg. loss: 0.174960\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.84, NNZs: 100, Bias: -1.630695, T: 56000, Avg. loss: 0.172032\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.72, NNZs: 100, Bias: -1.544728, T: 64000, Avg. loss: 0.168814\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.58, NNZs: 100, Bias: -1.494933, T: 72000, Avg. loss: 0.167573\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.46, NNZs: 100, Bias: -1.411579, T: 80000, Avg. loss: 0.166727\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1.35, NNZs: 100, Bias: -1.363212, T: 88000, Avg. loss: 0.164582\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1.27, NNZs: 100, Bias: -1.313608, T: 96000, Avg. loss: 0.162734\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1.19, NNZs: 100, Bias: -1.269340, T: 104000, Avg. loss: 0.161908\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1.11, NNZs: 100, Bias: -1.244630, T: 112000, Avg. loss: 0.161349\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1.05, NNZs: 100, Bias: -1.188865, T: 120000, Avg. loss: 0.160314\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 0.98, NNZs: 100, Bias: -1.154914, T: 128000, Avg. loss: 0.160067\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 0.92, NNZs: 100, Bias: -1.143708, T: 136000, Avg. loss: 0.159072\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 0.87, NNZs: 100, Bias: -1.108638, T: 144000, Avg. loss: 0.158714\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 0.83, NNZs: 100, Bias: -1.092791, T: 152000, Avg. loss: 0.157747\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 0.80, NNZs: 100, Bias: -1.077409, T: 160000, Avg. loss: 0.157152\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 20 epochs took 0.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2.85, NNZs: 100, Bias: -1.662258, T: 8000, Avg. loss: 0.517822\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.69, NNZs: 100, Bias: -1.641702, T: 16000, Avg. loss: 0.414737\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.68, NNZs: 100, Bias: -1.569327, T: 24000, Avg. loss: 0.402466\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2.63, NNZs: 100, Bias: -1.529905, T: 32000, Avg. loss: 0.399955\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2.59, NNZs: 100, Bias: -1.569156, T: 40000, Avg. loss: 0.394439\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2.58, NNZs: 100, Bias: -1.623041, T: 48000, Avg. loss: 0.393862\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2.62, NNZs: 100, Bias: -1.584225, T: 56000, Avg. loss: 0.392712\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2.62, NNZs: 100, Bias: -1.625088, T: 64000, Avg. loss: 0.389383\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 2.62, NNZs: 100, Bias: -1.616991, T: 72000, Avg. loss: 0.388226\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 2.59, NNZs: 100, Bias: -1.597989, T: 80000, Avg. loss: 0.385967\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 2.61, NNZs: 100, Bias: -1.574616, T: 88000, Avg. loss: 0.386885\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 2.59, NNZs: 100, Bias: -1.574030, T: 96000, Avg. loss: 0.386847\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2.61, NNZs: 100, Bias: -1.595564, T: 104000, Avg. loss: 0.385872\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2.58, NNZs: 100, Bias: -1.547628, T: 112000, Avg. loss: 0.384529\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 2.58, NNZs: 100, Bias: -1.579077, T: 120000, Avg. loss: 0.384676\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 2.57, NNZs: 100, Bias: -1.568177, T: 128000, Avg. loss: 0.384176\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 2.57, NNZs: 100, Bias: -1.600637, T: 136000, Avg. loss: 0.383434\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 2.56, NNZs: 100, Bias: -1.573810, T: 144000, Avg. loss: 0.383237\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 2.55, NNZs: 100, Bias: -1.588959, T: 152000, Avg. loss: 0.382246\n",
            "Total training time: 0.07 seconds.\n",
            "Convergence after 19 epochs took 0.07 seconds\n",
            "-- Epoch 1\n",
            "Norm: 3.11, NNZs: 100, Bias: -1.394575, T: 8000, Avg. loss: 0.331038\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3.02, NNZs: 100, Bias: -1.312181, T: 16000, Avg. loss: 0.264662\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.95, NNZs: 100, Bias: -1.307923, T: 24000, Avg. loss: 0.257911\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2.96, NNZs: 100, Bias: -1.276802, T: 32000, Avg. loss: 0.255258\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2.90, NNZs: 100, Bias: -1.291624, T: 40000, Avg. loss: 0.253140\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2.91, NNZs: 100, Bias: -1.311428, T: 48000, Avg. loss: 0.251335\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2.90, NNZs: 100, Bias: -1.305098, T: 56000, Avg. loss: 0.250743\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2.88, NNZs: 100, Bias: -1.291849, T: 64000, Avg. loss: 0.249753\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 2.87, NNZs: 100, Bias: -1.268114, T: 72000, Avg. loss: 0.248892\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 2.86, NNZs: 100, Bias: -1.268401, T: 80000, Avg. loss: 0.248747\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 2.85, NNZs: 100, Bias: -1.278959, T: 88000, Avg. loss: 0.248395\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 11 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5.43, NNZs: 100, Bias: -2.480362, T: 8000, Avg. loss: 0.071649\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.12, NNZs: 100, Bias: -2.624572, T: 16000, Avg. loss: 0.041984\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 4.93, NNZs: 100, Bias: -2.718594, T: 24000, Avg. loss: 0.035884\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.80, NNZs: 100, Bias: -2.724098, T: 32000, Avg. loss: 0.033768\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.68, NNZs: 100, Bias: -2.772029, T: 40000, Avg. loss: 0.032149\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.60, NNZs: 100, Bias: -2.752287, T: 48000, Avg. loss: 0.030842\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.52, NNZs: 100, Bias: -2.744052, T: 56000, Avg. loss: 0.030219\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.45, NNZs: 100, Bias: -2.753025, T: 64000, Avg. loss: 0.029530\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.39, NNZs: 100, Bias: -2.750259, T: 72000, Avg. loss: 0.028830\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.33, NNZs: 100, Bias: -2.771881, T: 80000, Avg. loss: 0.028510\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.27, NNZs: 100, Bias: -2.778538, T: 88000, Avg. loss: 0.028061\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 11 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 4.15, NNZs: 100, Bias: -2.304359, T: 8000, Avg. loss: 0.028092\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3.90, NNZs: 100, Bias: -2.340536, T: 16000, Avg. loss: 0.017200\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 3.74, NNZs: 100, Bias: -2.371460, T: 24000, Avg. loss: 0.014436\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 3.62, NNZs: 100, Bias: -2.389933, T: 32000, Avg. loss: 0.012797\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 3.53, NNZs: 100, Bias: -2.412421, T: 40000, Avg. loss: 0.011779\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 3.45, NNZs: 100, Bias: -2.426189, T: 48000, Avg. loss: 0.011007\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 3.38, NNZs: 100, Bias: -2.430247, T: 56000, Avg. loss: 0.010445\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 3.34, NNZs: 100, Bias: -2.405686, T: 64000, Avg. loss: 0.010041\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 3.30, NNZs: 100, Bias: -2.394482, T: 72000, Avg. loss: 0.009613\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 3.25, NNZs: 100, Bias: -2.405411, T: 80000, Avg. loss: 0.009333\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 10 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6.57, NNZs: 100, Bias: -1.459328, T: 8000, Avg. loss: 0.256154\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.92, NNZs: 100, Bias: -1.494760, T: 16000, Avg. loss: 0.128635\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.55, NNZs: 100, Bias: -1.530776, T: 24000, Avg. loss: 0.113362\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.29, NNZs: 100, Bias: -1.504456, T: 32000, Avg. loss: 0.106037\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.10, NNZs: 100, Bias: -1.494495, T: 40000, Avg. loss: 0.101630\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.95, NNZs: 100, Bias: -1.467284, T: 48000, Avg. loss: 0.098416\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.81, NNZs: 100, Bias: -1.484516, T: 56000, Avg. loss: 0.096654\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.72, NNZs: 100, Bias: -1.455023, T: 64000, Avg. loss: 0.094806\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.63, NNZs: 100, Bias: -1.466629, T: 72000, Avg. loss: 0.093437\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.56, NNZs: 100, Bias: -1.450936, T: 80000, Avg. loss: 0.093040\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.51, NNZs: 100, Bias: -1.429569, T: 88000, Avg. loss: 0.092105\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.46, NNZs: 100, Bias: -1.419613, T: 96000, Avg. loss: 0.091329\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.41, NNZs: 100, Bias: -1.423263, T: 104000, Avg. loss: 0.091074\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.37, NNZs: 100, Bias: -1.407749, T: 112000, Avg. loss: 0.090609\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 14 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5.90, NNZs: 100, Bias: -2.881358, T: 8000, Avg. loss: 0.046715\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.54, NNZs: 100, Bias: -3.121755, T: 16000, Avg. loss: 0.021504\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.33, NNZs: 100, Bias: -3.193492, T: 24000, Avg. loss: 0.017417\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.18, NNZs: 100, Bias: -3.217966, T: 32000, Avg. loss: 0.015565\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.05, NNZs: 100, Bias: -3.243857, T: 40000, Avg. loss: 0.014437\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.95, NNZs: 100, Bias: -3.254092, T: 48000, Avg. loss: 0.013471\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.86, NNZs: 100, Bias: -3.257881, T: 56000, Avg. loss: 0.012664\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.78, NNZs: 100, Bias: -3.262063, T: 64000, Avg. loss: 0.012036\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.71, NNZs: 100, Bias: -3.250668, T: 72000, Avg. loss: 0.011490\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.64, NNZs: 100, Bias: -3.250869, T: 80000, Avg. loss: 0.011124\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 10 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 3.01, NNZs: 100, Bias: -2.145579, T: 8000, Avg. loss: 0.033971\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.71, NNZs: 100, Bias: -2.133911, T: 16000, Avg. loss: 0.019616\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.56, NNZs: 100, Bias: -2.111293, T: 24000, Avg. loss: 0.015771\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2.46, NNZs: 100, Bias: -2.092249, T: 32000, Avg. loss: 0.013710\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2.39, NNZs: 100, Bias: -2.080121, T: 40000, Avg. loss: 0.012404\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2.34, NNZs: 100, Bias: -2.065315, T: 48000, Avg. loss: 0.011554\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2.30, NNZs: 100, Bias: -2.043176, T: 56000, Avg. loss: 0.010767\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2.26, NNZs: 100, Bias: -2.047001, T: 64000, Avg. loss: 0.010476\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 2.24, NNZs: 100, Bias: -2.039000, T: 72000, Avg. loss: 0.009864\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 2.22, NNZs: 100, Bias: -2.024054, T: 80000, Avg. loss: 0.009673\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 10 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2.62, NNZs: 100, Bias: -2.046187, T: 8000, Avg. loss: 0.078035\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.25, NNZs: 100, Bias: -1.991091, T: 16000, Avg. loss: 0.043765\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.03, NNZs: 100, Bias: -1.964073, T: 24000, Avg. loss: 0.039065\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.89, NNZs: 100, Bias: -1.904544, T: 32000, Avg. loss: 0.036463\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.78, NNZs: 100, Bias: -1.835686, T: 40000, Avg. loss: 0.035245\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.71, NNZs: 100, Bias: -1.759628, T: 48000, Avg. loss: 0.033962\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.63, NNZs: 100, Bias: -1.711370, T: 56000, Avg. loss: 0.033609\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.57, NNZs: 100, Bias: -1.662440, T: 64000, Avg. loss: 0.032939\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.51, NNZs: 100, Bias: -1.623566, T: 72000, Avg. loss: 0.032492\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.46, NNZs: 100, Bias: -1.580438, T: 80000, Avg. loss: 0.031942\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1.41, NNZs: 100, Bias: -1.552759, T: 88000, Avg. loss: 0.031620\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 11 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7.49, NNZs: 100, Bias: -2.669471, T: 8000, Avg. loss: 0.056876\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 7.12, NNZs: 100, Bias: -2.794889, T: 16000, Avg. loss: 0.034891\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 6.87, NNZs: 100, Bias: -2.850558, T: 24000, Avg. loss: 0.029703\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 6.68, NNZs: 100, Bias: -2.892304, T: 32000, Avg. loss: 0.026114\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.54, NNZs: 100, Bias: -2.918591, T: 40000, Avg. loss: 0.023643\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.41, NNZs: 100, Bias: -2.941320, T: 48000, Avg. loss: 0.021824\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.31, NNZs: 100, Bias: -2.954251, T: 56000, Avg. loss: 0.020444\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.21, NNZs: 100, Bias: -2.978776, T: 64000, Avg. loss: 0.019235\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.13, NNZs: 100, Bias: -2.997787, T: 72000, Avg. loss: 0.018189\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.05, NNZs: 100, Bias: -3.005195, T: 80000, Avg. loss: 0.017241\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.98, NNZs: 100, Bias: -3.025964, T: 88000, Avg. loss: 0.016530\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.92, NNZs: 100, Bias: -3.045691, T: 96000, Avg. loss: 0.015918\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.86, NNZs: 100, Bias: -3.052150, T: 104000, Avg. loss: 0.015197\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.80, NNZs: 100, Bias: -3.073309, T: 112000, Avg. loss: 0.014663\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 14 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 3.31, NNZs: 100, Bias: -1.593145, T: 8000, Avg. loss: 0.101325\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3.29, NNZs: 100, Bias: -1.751223, T: 16000, Avg. loss: 0.062909\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 3.34, NNZs: 100, Bias: -1.769411, T: 24000, Avg. loss: 0.058409\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 3.33, NNZs: 100, Bias: -1.811686, T: 32000, Avg. loss: 0.056937\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 3.33, NNZs: 100, Bias: -1.842197, T: 40000, Avg. loss: 0.056166\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 3.36, NNZs: 100, Bias: -1.828944, T: 48000, Avg. loss: 0.055385\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 3.36, NNZs: 100, Bias: -1.898060, T: 56000, Avg. loss: 0.055289\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 3.39, NNZs: 100, Bias: -1.884798, T: 64000, Avg. loss: 0.054633\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 3.43, NNZs: 100, Bias: -1.881029, T: 72000, Avg. loss: 0.054342\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 9 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2.61, NNZs: 100, Bias: -1.549767, T: 8000, Avg. loss: 0.086498\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.23, NNZs: 100, Bias: -1.589411, T: 16000, Avg. loss: 0.037478\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2.02, NNZs: 100, Bias: -1.577991, T: 24000, Avg. loss: 0.032649\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.89, NNZs: 100, Bias: -1.555146, T: 32000, Avg. loss: 0.029866\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.79, NNZs: 100, Bias: -1.523812, T: 40000, Avg. loss: 0.028623\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.71, NNZs: 100, Bias: -1.518221, T: 48000, Avg. loss: 0.027715\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.65, NNZs: 100, Bias: -1.500388, T: 56000, Avg. loss: 0.027103\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.59, NNZs: 100, Bias: -1.479395, T: 64000, Avg. loss: 0.026688\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.54, NNZs: 100, Bias: -1.464209, T: 72000, Avg. loss: 0.026441\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.50, NNZs: 100, Bias: -1.456396, T: 80000, Avg. loss: 0.026210\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 10 epochs took 0.03 seconds\n",
            "word2vec Score on Training dataset...\n",
            "\n",
            "\n",
            "accuracy: 0.674\n",
            "f1 score: 0.630\n",
            "word2vec Score on testing dataset...\n",
            "\n",
            "\n",
            "accuracy: 0.647\n",
            "f1 score: 0.600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    0.7s finished\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}